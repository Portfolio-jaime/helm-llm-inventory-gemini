# ğŸ“¦ Helm Inventory LLM (con Gemini)

Este proyecto permite consultar el inventario de componentes desplegados vÃ­a Helm en un clÃºster EKS de AWS, interactuando con un modelo LLM (Gemini) para realizar preguntas en lenguaje natural sobre el estado de la infraestructura.

---

## ğŸ–¼ï¸ Diagrama de arquitectura

```mermaid
flowchart LR
    user["Usuario (web UI)"] --> streamlit["Streamlit UI (web_ui.py)"]
    gemini -.-> gcp["Google Generative AI"]
```

---

## ğŸš€ Funcionalidades

- Inventario completo de releases Helm por clÃºster
- SelecciÃ³n de clÃºster EKS desde interfaz
- Cambio automÃ¡tico de contexto `kubectl` con perfil AWS
- ValidaciÃ³n de conexiÃ³n al clÃºster
- RevisiÃ³n de versiones instaladas de `kubectl`, `helm`, `aws-cli`
- ComparaciÃ³n con Ãºltimas versiones disponibles (GitHub)
- Consulta de versiones de nodos y del clÃºster
- Preguntas en lenguaje natural usando Gemini
- ExportaciÃ³n de recomendaciones a CSV y PDF
- Historial de preguntas y respuestas
- Interfaz web vÃ­a `Streamlit`

---

## ğŸ“‚ Estructura del proyecto

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ inventory.py         # ObtenciÃ³n del inventario de Helm
â”‚   â”œâ”€â”€ llm_gemini.py        # IntegraciÃ³n con modelo Gemini
â”‚   â”œâ”€â”€ tools_info.py        # Utilidades: versiones, validaciones
â”‚   â”œâ”€â”€ chart_versions.py    # Funciones utilitarias para versiones y PDF (sin Streamlit)
â”‚   â””â”€â”€ web_ui.py            # Interfaz principal Streamlit (Ãºnico archivo con UI)
â”œâ”€â”€ .env                     # ConfiguraciÃ³n de entorno
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> **Nota:** Solo `web_ui.py` contiene la interfaz Streamlit. El resto de archivos son utilidades y lÃ³gica.

---

## ğŸ§ª Requisitos

- Python 3.10+
- Acceso a AWS CLI y EKS
- Clave de API de Google Generative AI (Gemini)
- `kubectl` y `helm` instalados

---

## ğŸš¦ GuÃ­a rÃ¡pida de ejecuciÃ³n

1. **Clona el repositorio:**
   ```bash
   git clone <repo-url>
   cd <repo>
   ```
2. **Crea el archivo `.env`** con tus credenciales y configuraciÃ³n:
   ```env
   # Ejemplo en .env
   GEMINI_API_KEY=...
   MCP_SERVER_URL=http://localhost:8000/mcp
   OPENAI_API_KEY=...
   OPENAI_MODEL=gpt-3.5-turbo
   EKS_CLUSTERS_JSON={...}
   EKS_PROFILES_JSON={...}
   ```
3. **Instala las dependencias:**
   ```bash
   pip install -r requirements.txt
   ```
4. **Ejecuta la aplicaciÃ³n web:**
   ```bash
   PYTHONPATH=. streamlit run app/web_ui.py
   ```
5. **Accede a la interfaz:**
   Abre tu navegador en [http://localhost:8501](http://localhost:8501)

---

## ğŸ’¬ Ejemplos de preguntas

- Â¿QuÃ© versiÃ³n tiene Prometheus?
- Â¿EstÃ¡ Loki desplegado?
- Â¿QuÃ© componentes estÃ¡n desactualizados?
- Â¿QuÃ© versiÃ³n tiene metrics-server?

---

## ğŸ“¤ Exportar recomendaciones

En la pestaÃ±a de recomendaciones puedes exportar los resultados a CSV o PDF usando los botones de descarga.

---

## ğŸ“Œ Notas tÃ©cnicas

- El clÃºster seleccionado cambia automÃ¡ticamente el contexto de kubectl usando AWS CLI.
- La validaciÃ³n de acceso se hace con `kubectl get nodes`.
- Gemini se consulta vÃ­a `google-generativeai` y el modelo `gemini-1.5-pro-latest`.
- El archivo `chart_versions.py` solo contiene funciones utilitarias, no debe tener UI de Streamlit.

---

## ğŸ”’ Seguridad

No se guarda informaciÃ³n sensible. Los perfiles de AWS y claves deben manejarse mediante `.env`.

---

## ğŸ“¥ Futuras mejoras

- Soporte para modelos locales (Ollama)
- Mejoras en la exportaciÃ³n y visualizaciÃ³n de reportes

